{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep XOR #\n",
    "\n",
    "The goal of this notebook is to illustrate, step by step, the modelling of an [XOR gate](https://en.wikipedia.org/wiki/Exclusive_or) using a simple neural network trained via gradient descent.\n",
    "\n",
    "To have a better understanding of neural networks, I decided go through the derivation of all the necessary equations, and implement them with [numpy](http://www.numpy.org/). \n",
    "Hopefully it is helpful to other practitioners too. Life is sharing (openly on github) :)\n",
    "\n",
    "(The whole thing is inspired by Section 6.2 of the absolutely fantastic [The Deep Learning Book](http://www.deeplearningbook.org/), and [this](http://cs231n.github.io/neural-networks-case-study/#net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np  # That's all we need :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data ##\n",
    "\n",
    "We randomly generate a handful of examples to later train our model. \n",
    "There are only 4 different unique input combinations to XOR:\n",
    "* $0 \\oplus 0 = 0$\n",
    "* $0 \\oplus 1 = 1$\n",
    "* $1 \\oplus 0 = 1$\n",
    "* $1 \\oplus 1 = 0$\n",
    "\n",
    "$N$ observations of pairs of ones and zeroes are generated and stored in $X \\in \\mathbb{N}^{N \\times 2}$, and passed through the XOR operation to obtain the target set of labels $Y \\in \\mathbb{N}^{N \\times 2}$, which we encode as one-hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 XOR 1 = 0\n",
      "0 XOR 1 = 1\n",
      "0 XOR 1 = 1\n",
      "0 XOR 0 = 0\n",
      "0 XOR 1 = 1\n",
      "0 XOR 0 = 0\n",
      "1 XOR 0 = 1\n",
      "1 XOR 0 = 1\n",
      "0 XOR 0 = 0\n",
      "0 XOR 0 = 0\n"
     ]
    }
   ],
   "source": [
    "def gen_data(N):\n",
    "    \"\"\"Generates an XOR dataset using one-hot vector encoding.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    N: int\n",
    "        Number of observations to be generated.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X: np.ndarray((N, 2))\n",
    "        Matrix containing an observation per row, representing the input\n",
    "        to the XOR operation (observations).\n",
    "    Y: np.ndarray((N, 2))\n",
    "        Result of the XOR operation for each row, encoded with \n",
    "        one-hot vector per row (targets).\n",
    "    \"\"\"\n",
    "    X = np.random.randint(0, 2, size=(N, 2))\n",
    "    Y = np.zeros((N, 2))\n",
    "    Y[np.arange(N), [int(np.logical_xor(x[0], x[1])) for x in X]] = 1\n",
    "    return X, Y\n",
    "\n",
    "def xor_print(X, Y, N):\n",
    "    \"\"\"Prints out an XOR dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray((N, 2))\n",
    "        Observations.\n",
    "    Y: np.ndarray((N, 2))\n",
    "        Targets.\n",
    "    N: int\n",
    "        Number of observations to be printed.\n",
    "    \"\"\"\n",
    "    [print(str(x[0]) + ' XOR ' + str(x[1]) + ' = ' + str(np.argmax(y))) \n",
    "     for x, y in zip(X[:N], Y[:N])]\n",
    "\n",
    "# Create some training data\n",
    "N = 100\n",
    "X, Y = gen_data(N)\n",
    "xor_print(X, Y, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model ##\n",
    "\n",
    "To approximate the XOR function we need a non-linear model. \n",
    "Since neural networks are so [hot](http://i.imgur.com/FNoToht.jpg) nowardays, we design a feedforward neural network with a single hidden layer.\n",
    "\n",
    "The network has two output units $\\hat{y}_1$ and $\\hat{y}_2$, representing the likelihood of having a 0 or a 1, respectively, thus framing the problem as **classification**. The network diagram may look as follows:\n",
    "![Feedforward Neural Network2](./diagram_sm.png)\n",
    "\n",
    "Where the weights $W_1, W_2$ and basis $b_1, b_2$ are the coefficients to be learned, $\\mathbf{x} = \\{x_1, x_2\\}$ is the input, $\\mathbf{h} = \\{h_1, h_2\\}$ is the content of the hidden layer, and $\\mathbf{\\hat{y}} = \\{\\hat{y}_1, \\hat{y}_2\\}$ is the output of the network.\n",
    "\n",
    "We use a rectified linear unit (ReLU) for the activation of the hidden layer. Formally:\n",
    "\n",
    "$$\\mathbf{h} = \\mbox{max}\\{\\mathbf{0}, \\mathbf{x}^T W_1 + b_1\\}$$\n",
    "\n",
    "For the output layer, we use the standard softmax function, which is the common activation used for classification problems:\n",
    "\n",
    "$$\\mathbf{\\hat{y}} = \\frac{\\exp(\\mathbf{h}^T W_2 + b_2)}{\\sum_j^2\\exp(\\mathbf{h}^T W_2 + b_2)_j}$$\n",
    "\n",
    "### Multiple Inputs ###\n",
    "\n",
    "If we focus on multiple observations $X = \\{\\mathbf{x}_1^T, \\mathbf{x}_2^T, \\ldots, \\mathbf{x}_N^T\\}$ instead of a single one $\\mathbf{x}_i$ we can rewrite the equations as follows:\n",
    "\n",
    "$$H = \\mbox{max}\\{\\mathbf{0}, X W_1 + b_1\\}$$\n",
    "\n",
    "$$\\hat{Y} = \\frac{\\exp(H W_2 + b_2)}{\\sum_j^2\\exp(H W_2 + b_2)_j}$$\n",
    "\n",
    "where the biases $b_1, b_2$ are broadcasted across their respective inputs.\n",
    "\n",
    "We can now implement a function that computes a multiple input forward pass of the network using numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_pass(X, W1, W2, b1, b2):\n",
    "    \"\"\"Computes a forward pass of the network. Once the coefficients are trained,\n",
    "    this should compute the XOR on all x \\in X.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray((N, 2))\n",
    "        Inputs to the network.\n",
    "    W1: np.ndarray((2, n_hidden))\n",
    "        Set of weights for the first layer.\n",
    "    W2: np.ndarray((n_hidden, 2))\n",
    "        Set of weights for the second layer.\n",
    "    b1: float\n",
    "        Bias for the first layer.\n",
    "    b3: float\n",
    "        Bias for the second layer.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    H: np.ndarray((N, n_hidden))\n",
    "        Content of the hidden layer.\n",
    "    Y_est: np.ndarray((N, 2))\n",
    "        Output of the network, the two columns represent the likelihood of\n",
    "        having a 0 or a 1, in this order.\n",
    "    \"\"\"\n",
    "    # Hidden layer forward pass\n",
    "    H = np.maximum(0, np.dot(X, W1) + b1)  # ReLU activation\n",
    "    \n",
    "    # Second layer forward pass without activation\n",
    "    linear = np.dot(H, W2) + b2\n",
    "    \n",
    "    # Softmax activation (output)\n",
    "    Y_est = np.exp(linear) / np.sum(np.exp(linear), axis=1, keepdims=True)  # [N x 2]\n",
    "    return H, Y_est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights and Biases Initialization ##\n",
    "\n",
    "We can initialize our training coefficients randomly. Notice how it would be straightforward to include more hidden units in our hidden layer. Feel free to play with this parameter, it seems to converge faster with 3 or 4 units. But for now, let's stick to our original model of 2 hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input -> Hidden layer weights and bias\n",
    "n_hidden = 2\n",
    "W1 = np.random.random(size=(2, n_hidden))\n",
    "b1 = np.zeros((1, n_hidden)) # bias\n",
    "\n",
    "# Hidden -> Output layer weights and bias\n",
    "K = 2  # Either 0 or 1\n",
    "W2 = np.random.random(size=(n_hidden, K))\n",
    "b2 = np.zeros((1, K)) # bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function ##\n",
    "\n",
    "Next step is to define an appropriate differentiable loss function that allows us to train our network. The loss should be small when we are obtaining successful results with the current parameters, and high otherwise.\n",
    "\n",
    "The **cross-entropy** function is commonly used in classification problems, since its derivative is quite simple and yet successfully captures the cost of the network for a given set of parameters.\n",
    "\n",
    "The average cross-entropy is defined as:\n",
    "\n",
    "$$L = - \\frac{1}{N}\\sum_{i}^N \\mathbf{y}_i \\odot \\log(\\mathbf{\\hat{y}}_i) $$\n",
    "\n",
    "where $\\odot$ is the element-wise product.\n",
    "\n",
    "Let's quickly inspect this function. If we would have the perfect prediction (e.g., $\\mathbf{y}_i = \\{0, 1\\}$ and $\\mathbf{\\hat{y}}_i = \\{0, 1\\}$), then $L_i$ would be 0, as desired. Whereas, if we would have a completely wrong one (e.g., $\\mathbf{y}_i = \\{0, 1\\}$ and $\\mathbf{\\hat{y}}_i = \\{1, 0\\}$), then we would get $L_i = \\infty$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss(Y, Y_est):\n",
    "    \"\"\"Computes the averate cross-entropy loss.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Y: np.ndarray((N, 2))\n",
    "        Target labels (XOR correct results).\n",
    "    Y: np.ndarray((N, 2))\n",
    "        Estimated XOR values.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    loss: float\n",
    "        Loss value: the average cross-entropy\n",
    "    \"\"\"\n",
    "    N = Y.shape[0]\n",
    "    return - np.sum(np.log(np.sum(Y_est * Y, axis=1))) / float(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch 0: loss 0.706469\t acc 52.0%\n",
      "\tEpoch 1000: loss 0.649812\t acc 50.0%\n",
      "\tEpoch 2000: loss 0.521621\t acc 67.0%\n",
      "\tEpoch 3000: loss 0.331217\t acc 100.0%\n",
      "\tEpoch 4000: loss 0.186399\t acc 100.0%\n",
      "\tEpoch 5000: loss 0.111837\t acc 100.0%\n",
      "\tEpoch 6000: loss 0.071910\t acc 100.0%\n",
      "\tEpoch 7000: loss 0.055673\t acc 100.0%\n",
      "\tEpoch 8000: loss 0.038435\t acc 100.0%\n",
      "\tEpoch 9000: loss 0.029984\t acc 100.0%\n",
      "\tEpoch 10000: loss 0.026018\t acc 100.0%\n",
      "\tEpoch 11000: loss 0.022100\t acc 100.0%\n",
      "\tEpoch 12000: loss 0.019636\t acc 100.0%\n",
      "\tEpoch 13000: loss 0.018815\t acc 100.0%\n",
      "\tEpoch 14000: loss 0.015000\t acc 100.0%\n",
      "\tEpoch 15000: loss 0.012655\t acc 100.0%\n",
      "\tEpoch 16000: loss 0.011165\t acc 100.0%\n",
      "\tEpoch 17000: loss 0.011422\t acc 100.0%\n",
      "\tEpoch 18000: loss 0.010205\t acc 100.0%\n",
      "\tEpoch 19000: loss 0.008463\t acc 100.0%\n",
      "\tEpoch 20000: loss 0.007935\t acc 100.0%\n",
      "\tEpoch 21000: loss 0.009081\t acc 100.0%\n",
      "\tEpoch 22000: loss 0.007036\t acc 100.0%\n",
      "\tEpoch 23000: loss 0.007746\t acc 100.0%\n",
      "\tEpoch 24000: loss 0.006187\t acc 100.0%\n",
      "\tEpoch 25000: loss 0.006515\t acc 100.0%\n",
      "\tEpoch 26000: loss 0.005928\t acc 100.0%\n",
      "\tEpoch 27000: loss 0.004838\t acc 100.0%\n",
      "\tEpoch 28000: loss 0.005227\t acc 100.0%\n",
      "\tEpoch 29000: loss 0.005066\t acc 100.0%\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 30000\n",
    "step = 0.01\n",
    "for n_epoch in range(n_epochs):\n",
    "    # Forward pass of the network, catching the two layers' outputs\n",
    "    H, Y_est = forward_pass(X, W1, W2, b1, b2)\n",
    "    \n",
    "    # Loss: average cross-entropy loss\n",
    "    loss = compute_loss(Y, Y_est)\n",
    "    \n",
    "    # Accuracy\n",
    "    acc = np.sum(Y[np.arange(N), np.argmax(Y_est, axis=1)]) / float(N)\n",
    "    \n",
    "    # Printout\n",
    "    if n_epoch % 1000 == 0:\n",
    "        print(\"\\tEpoch %d: loss %f\\t acc %.1f%%\" % (n_epoch, loss, (acc * 100)))\n",
    "        \n",
    "    # Gradient of the second (output) layer\n",
    "    dpreds = np.copy(Y_est)\n",
    "    dpreds[np.where(Y == 1)] -= 1\n",
    "    dpreds /= float(N)\n",
    "    \n",
    "    # Gradient of the hidden layer\n",
    "    dhidden = np.dot(dpreds, W2.T)\n",
    "    dhidden[H <= 0] = 0  # ReLU backprop\n",
    "    \n",
    "    # Backpropagation of output layer\n",
    "    dW2 = np.dot(H.T, dpreds)\n",
    "    db2 = np.sum(dpreds, axis=0, keepdims=True)\n",
    "    \n",
    "    # Backprop of hidden layer\n",
    "    dW1 = np.dot(X.T, dhidden)\n",
    "    db1 = np.sum(dhidden, axis=0, keepdims=True)\n",
    "  \n",
    "    # Update step\n",
    "    W1 -= step * dW1\n",
    "    b1 -= step * db1\n",
    "    W2 -= step * dW2\n",
    "    b2 -= step * db2\n",
    "    \n",
    "    # Generate some new training data (this may help to avoid falling into local minima)\n",
    "    X, Y = gen_data(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in Test set: 100.00%\n"
     ]
    }
   ],
   "source": [
    "X_test, Y_test = gen_data(N)\n",
    "_, Y_est = forward_pass(X_test, W1, W2, b1, b2)\n",
    "acc = np.sum(Y_test[np.arange(N), np.argmax(Y_est, axis=1)]) / float(N)\n",
    "print(\"Accuracy in Test set: %.2f%%\" % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normal equations, solving with these four examples:\n",
    "X = np.array([[1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]])  # Added the bias as ones in the first column\n",
    "y = np.array([0, 1, 1, 0])\n",
    "b, w1, w2 = np.dot(np.linalg.inv(np.dot(X.T, X)), np.dot(X.T, y))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
