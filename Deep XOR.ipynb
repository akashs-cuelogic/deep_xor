{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep XOR #\n",
    "\n",
    "The goal of this notebook is to illustrate, step by step, the modelling of an [XOR gate](https://en.wikipedia.org/wiki/Exclusive_or) using a simple neural network trained via gradient descent.\n",
    "\n",
    "This is basically a little exercise I decided to go through to have a better understanding of how neural networks work. Here you'll find the derivation of all necessary equations and their implementation in [numpy](http://www.numpy.org/). \n",
    "Hopefully it may help other practitioners too. Life is sharing (openly on github) :)\n",
    "\n",
    "(The whole thing is inspired by Section 6.2 of the absolutely fantastic [The Deep Learning Book](http://www.deeplearningbook.org/), and [this](http://cs231n.github.io/neural-networks-case-study/#net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np  # That's all we need :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data ##\n",
    "\n",
    "We randomly generate a handful of examples to later train our model. \n",
    "There are only 4 different unique input combinations to XOR:\n",
    "* $0 \\oplus 0 = 0$\n",
    "* $0 \\oplus 1 = 1$\n",
    "* $1 \\oplus 0 = 1$\n",
    "* $1 \\oplus 1 = 0$\n",
    "\n",
    "Thus, $N$ observations randomly sampled from these 4 pairs of ones and zeroes are generated and stored in $X \\in \\mathbb{N}^{N \\times 2}$, and passed through the XOR operation to obtain the target set of labels $Y \\in \\mathbb{N}^{N \\times 2}$, which we encode as one-hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 XOR 0 = 1\n",
      "1 XOR 1 = 0\n",
      "1 XOR 1 = 0\n",
      "0 XOR 1 = 1\n",
      "1 XOR 0 = 1\n",
      "1 XOR 1 = 0\n",
      "1 XOR 1 = 0\n",
      "0 XOR 0 = 0\n",
      "1 XOR 0 = 1\n",
      "1 XOR 1 = 0\n"
     ]
    }
   ],
   "source": [
    "def gen_data(N):\n",
    "    \"\"\"Generates an XOR dataset using one-hot vector encoding.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    N: int\n",
    "        Number of observations to be generated.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X: np.ndarray((N, 2))\n",
    "        Matrix containing an observation per row, representing the input\n",
    "        to the XOR operation (observations).\n",
    "    Y: np.ndarray((N, 2))\n",
    "        Result of the XOR operation for each row, encoded with \n",
    "        one-hot vector per row (targets).\n",
    "    \"\"\"\n",
    "    X = np.random.randint(0, 2, size=(N, 2))\n",
    "    Y = np.zeros((N, 2))\n",
    "    Y[np.arange(N), [int(np.logical_xor(x[0], x[1])) for x in X]] = 1\n",
    "    return X, Y\n",
    "\n",
    "def xor_print(X, Y, N):\n",
    "    \"\"\"Prints out an XOR dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray((N, 2))\n",
    "        Observations.\n",
    "    Y: np.ndarray((N, 2))\n",
    "        Targets.\n",
    "    N: int\n",
    "        Number of observations to be printed.\n",
    "    \"\"\"\n",
    "    [print(str(x[0]) + ' XOR ' + str(x[1]) + ' = ' + str(np.argmax(y))) \n",
    "     for x, y in zip(X[:N], Y[:N])]\n",
    "\n",
    "# Create some training data\n",
    "N = 100\n",
    "X, Y = gen_data(N)\n",
    "xor_print(X, Y, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model ##\n",
    "\n",
    "To approximate the XOR function, which is non-linear, we need a non-linear model. \n",
    "Since neural networks are so [cool](http://i.imgur.com/FNoToht.jpg), we design a feedforward neural network with a single hidden layer (ok, that's not too *deep*, but nowadays it seems almost impossible to write something interesting without this magic [word](https://cdn.meme.am/instances/500x/72561253.jpg) in it, hence the title of this notebook).\n",
    "\n",
    "The network has two output units $\\hat{y}_1$ and $\\hat{y}_2$, representing the likelihood of having a 0 or a 1, respectively, thus framing the problem as **classification**. The network diagram may look as follows:\n",
    "![Feedforward Neural Network2](./diagram_sm.png)\n",
    "\n",
    "Where the weights $W_1, W_2$ and basis $b_1, b_2$ are the coefficients to be learned, $\\mathbf{x} = \\{x_1, x_2\\}$ is the input, $\\mathbf{h} = \\{h_1, h_2\\}$ is the content of the hidden layer, and $\\mathbf{\\hat{y}} = \\{\\hat{y}_1, \\hat{y}_2\\}$ is the output of the network.\n",
    "\n",
    "Each layer is composed by a standard linear equation plus its non-linear activation. We use a rectified linear unit (ReLU) $\\sigma$ for the activation of the hidden layer. Formally:\n",
    "\n",
    "$$\\mathbf{h} = \\sigma(\\mathbf{x}^T W_1 + b_1) = \\mbox{max}\\{\\mathbf{0}, \\mathbf{x}^T W_1 + b_1\\}$$\n",
    "\n",
    "For the output layer, we use the standard softmax function, which is the common activation used for classification problems. To simplify the math, we decompose the output layer into its linear equation and its activation:\n",
    "\n",
    "$$\\mathbf{z} = \\mathbf{h}^T W_2 + b_2$$\n",
    "\n",
    "$$\\mathbf{\\hat{y}} = \\frac{e^\\mathbf{z}}{\\sum_j^2 e^{\\mathbf{z}_j}}$$\n",
    "\n",
    "Note how the denominator of the softmax function sums only twice: once for each class (i.e., 0 and 1). This function essentially squashes the predictions and then normalizes them, thus yielding a set of proper probabilities.\n",
    "\n",
    "### Multiple Inputs ###\n",
    "\n",
    "If we focus on multiple observations $X = \\{\\mathbf{x}_1^T, \\mathbf{x}_2^T, \\ldots, \\mathbf{x}_N^T\\}$ instead of a single one $\\mathbf{x}_i$ we can rewrite the equations as follows:\n",
    "\n",
    "$$H = \\sigma(X W_1 + b_1) = \\mbox{max}\\{\\mathbf{0}, X W_1 + b_1\\}$$\n",
    "\n",
    "$$Z = H W_2 + b_2$$\n",
    "\n",
    "$$\\hat{Y} = \\frac{e^Z}{\\sum_j^2 e ^{\\mathbf{z}_j}}$$\n",
    "\n",
    "where the biases $b_1, b_2$ are broadcasted across their respective inputs.\n",
    "\n",
    "We can now implement a function that computes a multiple input forward pass of the network using numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_pass(X, W1, W2, b1, b2):\n",
    "    \"\"\"Computes a forward pass of the network. Once the coefficients are trained,\n",
    "    this should compute the XOR on all x \\in X.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray((N, 2))\n",
    "        Inputs to the network.\n",
    "    W1: np.ndarray((2, n_hidden))\n",
    "        Set of weights for the first layer.\n",
    "    W2: np.ndarray((n_hidden, 2))\n",
    "        Set of weights for the second layer.\n",
    "    b1: float\n",
    "        Bias for the first layer.\n",
    "    b3: float\n",
    "        Bias for the second layer.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    H: np.ndarray((N, n_hidden))\n",
    "        Content of the hidden layer.\n",
    "    Y_est: np.ndarray((N, 2))\n",
    "        Output of the network, the two columns represent the likelihood of\n",
    "        having a 0 or a 1, in this order.\n",
    "    \"\"\"\n",
    "    # Hidden layer forward pass\n",
    "    H = np.maximum(0, np.dot(X, W1) + b1)  # ReLU activation\n",
    "    \n",
    "    # Second layer forward pass without activation\n",
    "    linear = np.dot(H, W2) + b2\n",
    "    \n",
    "    # Softmax activation (output)\n",
    "    Y_est = np.exp(linear) / np.sum(np.exp(linear), axis=1, keepdims=True)  # [N x 2]\n",
    "    return H, Y_est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Initialization ##\n",
    "\n",
    "We can initialize our training coefficients randomly (there are [fancier](http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf) ways, but this problem is not that sophisticated after all). Notice how it would be straightforward to include more hidden units in our hidden layer by just changing the `n_hidden` var. Feel free to play with this parameter, it seems to converge faster with 3 or 4 units. But for now, let's stick to our original model of 2 hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input -> Hidden layer weights and bias\n",
    "n_hidden = 2\n",
    "W1 = np.random.random(size=(2, n_hidden))\n",
    "b1 = np.zeros((1, n_hidden)) # bias\n",
    "\n",
    "# Hidden -> Output layer weights and bias\n",
    "K = 2  # Either 0 or 1\n",
    "W2 = np.random.random(size=(n_hidden, K))\n",
    "b2 = np.zeros((1, K)) # bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Loss Function ##\n",
    "\n",
    "Next step is to define an appropriate differentiable loss function that allows us to train our network. The loss should be small when we are obtaining successful results with the current parameters, and high otherwise.\n",
    "\n",
    "The **cross-entropy** function is commonly used in classification problems, since its derivative is quite simple and yet successfully captures the cost of the network for a given set of parameters.\n",
    "\n",
    "The average cross-entropy is defined as:\n",
    "\n",
    "$$L = - \\frac{1}{N}\\sum_{i}^N \\sum_{j}^2 y_{ij} \\log(\\hat{y}_{ij}) $$\n",
    "\n",
    "Let's quickly inspect this function. If we have a perfect prediction (e.g., $\\mathbf{y}_i = \\{0, 1\\}$ and $\\mathbf{\\hat{y}}_i = \\{0, 1\\}$), then $L_i$ is 0, as desired. Whereas, if we have completely wrong one (e.g., $\\mathbf{y}_i = \\{0, 1\\}$ and $\\mathbf{\\hat{y}}_i = \\{1, 0\\}$), then we get $L_i = \\infty$, also as required. Alrighten.\n",
    "\n",
    "We can now implement this function in numpy. To avoid the instability of computing $\\log(0)$, we can simply multiply each of the predicted results with the target value, and sum across them (one will be zero). This is essentially like \"selecting\" the likelihood of the correct class:\n",
    "\n",
    "$$L = - \\frac{1}{N}\\sum_{i}^N y_{ik} \\log(\\hat{y}_{ik}) + (1 - y_{ik}) \\log(\\hat{y}_{in}) = - \\frac{1}{N}\\sum_{i}^N \\log(\\hat{y}_{ik}) \\qquad\\text{where}\\qquad y_{ik} = 1$$\n",
    "\n",
    "And here its implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss(Y, Y_est):\n",
    "    \"\"\"Computes the averate cross-entropy loss.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Y: np.ndarray((N, 2))\n",
    "        Target labels (XOR correct results).\n",
    "    Y_est: np.ndarray((N, 2))\n",
    "        Estimated XOR values.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    loss: float\n",
    "        Loss value: the average cross-entropy.\n",
    "    \"\"\"\n",
    "    N = Y.shape[0]\n",
    "    return - np.sum(np.log(np.sum(Y * Y_est, axis=1))) / float(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Training ##\n",
    "\n",
    "As it is common in neural networks, we use [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) to train our set of weights $W_1, W2$ and biases $b_1, b_2$.\n",
    "This method makes use of the partial derivates of the whole model to identify the steepest direction (the gradient) of each trainable variable in order to minimize, step by step, the given loss function $L$.\n",
    "\n",
    "The [chain-rule](http://www.sosmath.com/calculus/diff/der04/der04.html) formula will help us in this process, *backpropagating* the gradient from the output of the network to its input. So, let us begin.\n",
    "\n",
    "### Loss Derivative ###\n",
    "\n",
    "The ([fun](http://jokideo.com/wp-content/uploads/meme/2014/06/Reaction-Pic---not-funny.jpg)) derivative of our loss function with respect to the softmax output of our model is:\n",
    "\n",
    "$$\\begin{split}\n",
    "\\frac{\\partial L}{\\partial z_{ij}} & = \\frac{\\partial L}{\\partial \\hat{y}_{ij}} \\frac{\\partial \\hat{y}_{ij}}{\\partial z_{ij}} \\\\\n",
    "& = \\frac{\\partial}{\\partial\\hat{y}_{ij}} \\left( - \\sum_{j}^2 y_{ij} \\log(\\hat{y}_{ij}) \\right) \\frac{\\partial \\hat{y}_{ij}}{\\partial z_{ij}}\\\\\n",
    "& = - \\sum_{j}^2 y_{ij} \\frac{1}{\\hat{y}_{ij}} \\frac{\\partial \\hat{y}_{ij}}{\\partial z_{ij}} \\\\\n",
    "& = - \\sum_{j}^2 y_{ij} \\frac{1}{\\hat{y}_{ij}} \\frac{\\partial}{\\partial z_{ij}} \\frac{e^{z_{ij}}}{\\sum_m^2e^{z_{im}}} \\\\\n",
    "& = - \\sum_{j}^2 y_{ij} \\frac{1}{\\hat{y}_{ij}} \\frac{e^{z_{ij}}\\sum_m^2 e^{z_{im}} - e^{z_{ij}}e^{z_{ij}}}{(\\sum_m^2e^{z_{im}})^2} \\\\\n",
    "& = - y_{ik} \\frac{1}{\\hat{y}_{ik}} \\frac{e^{z_{ik}}\\sum_m^2 e^{z_{im}} - e^{z_{ik}}e^{z_{ik}}}{(\\sum_m^2e^{z_{im}})^2} - (1 - y_{ik}) \\frac{1}{\\hat{y}_{in}} \\frac{e^{z_{in}}\\sum_m^2 e^{z_{im}} - e^{z_{in}}e^{z_{in}}}{(\\sum_m^2e^{z_{im}})^2}\\\\\n",
    "& = - y_{ik} \\frac{1}{\\hat{y}_{ik}} \\frac{e^{z_{ik}}}{\\sum_m^2e^{z_{im}}} \\frac{\\sum_m^2 e^{z_{im}} - e^{z_{ik}}}{\\sum_m^2e^{z_{im}}}\\\\\n",
    "& = - y_{ik} \\frac{1}{\\hat{y}_{ik}} \\hat{y}_{ik} \\frac{\\sum_m^2 e^{z_{im}} - e^{z_{ik}}}{\\sum_m^2e^{z_{im}}} \\\\\n",
    "& = - y_{ik} \\left( \\frac{\\sum_m^2 e^{z_{im}}}{\\sum_m^2e^{z_{im}}} - \\frac{e^{z_{ik}}}{\\sum_m^2e^{z_{im}}} \\right)\\\\\n",
    "& = - y_{ik} ( 1 - \\hat{y}_{ik})\\\\\n",
    "& = - y_{ik} + y_{ik}\\hat{y}_{ik}\\\\\n",
    "& = - y_{ij} + 1\\hat{y}_{ij}\\\\\n",
    "& = \\hat{y}_{ij} - y_{ij}\\\\\n",
    "\\end{split}$$\n",
    "\n",
    "Applied to all our datapoints, the result would be:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{z}_{j}} = - \\frac{1}{N} \\frac{\\partial}{\\partial \\mathbf{z}_{j}} \\sum_{j}^2 \\mathbf{y}_{j} \\odot \\log(\\mathbf{\\hat{y}}_{j}) = \\frac{\\mathbf{\\hat{y}}_{j} - \\mathbf{y}_{j}}{N}$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial Z} = \\frac{\\hat{Y} - Y}{N}$$\n",
    "\n",
    "where $\\odot$ is the element-wise product. Let's implement this in numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss_diff(Y, Y_est):\n",
    "    \"\"\"Computes the averate cross-entropy loss derivative.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Y: np.ndarray((N, 2))\n",
    "        Target labels (XOR correct results).\n",
    "    Y: np.ndarray((N, 2))\n",
    "        Estimated XOR values.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    loss_deriv: float\n",
    "        Loss value: the average cross-entropy derivative.\n",
    "    \"\"\"\n",
    "    return (Y_est - Y) / float(Y.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient of the hidden layer\n",
    "\n",
    "We have computed the partial derivative with respect to our last layer, now let's do the same with do it with respect to our hidden layer. This will help us when obtaining the partial derivatives to our trainable coefficients, which is ultimately what we want to obtain.\n",
    "\n",
    "Again, let's apply the chain-rule to derive this:\n",
    "\n",
    "$$\\begin{split}\n",
    "\\frac{\\partial L}{\\partial H} & = \\frac{\\partial L}{\\partial \\hat{Y}} \\frac{\\partial \\hat{Y}}{\\partial Z} \\frac{\\partial Z}{\\partial H} \\\\\n",
    "& = \\frac{\\hat{Y} - Y}{N} \\frac{\\partial Z}{\\partial H} \\\\\n",
    "& = \\frac{\\hat{Y} - Y}{N} \\frac{\\partial}{\\partial H} H W_2 + b_2 \\\\\n",
    "& = \\frac{\\hat{Y} - Y}{N} W_2^T\\\\\n",
    "\\end{split}$$\n",
    "\n",
    "Including the ReLU activation function $\\sigma$ from the first layer will simplify the math later for backpropagation, let's do it:\n",
    "\n",
    "$$\\begin{split}\n",
    "\\frac{\\partial L}{\\partial \\sigma} & = \\frac{\\partial L}{\\partial \\hat{Y}} \\frac{\\partial \\hat{Y}}{\\partial Z} \\frac{\\partial Z}{\\partial H}\\frac{\\partial H}{\\partial \\sigma} \\\\\n",
    "& = \\frac{\\hat{Y} - Y}{N} W_2^T \\frac{\\partial H}{\\partial \\sigma}\\\\\n",
    "& = \\frac{\\hat{Y} - Y}{N} W_2^T \\frac{\\partial}{\\partial \\sigma}\\sigma(X W_1 + b_1)\\\\\n",
    "& = \\frac{\\hat{Y} - Y}{N} W_2^T \\frac{\\partial}{\\partial \\sigma}\\mbox{max}\\{\\mathbf{0}, X W_1 + b_1\\}\\\\\n",
    "& = \\frac{\\hat{Y} - Y}{N} W_2^T \\mathbb{1}\\{X W_1 + b_1 > 0\\}\\\\\n",
    "\\end{split}$$\n",
    "\n",
    "As you can see, the derivative of a ReLu function $\\mbox{max}\\{0, x\\}$ is simply the indicator $\\mathbb{1}\\{x > 0\\}$. If this is not obvious, here's a [further explanation](http://kawahara.ca/what-is-the-derivative-of-relu/).\n",
    "\n",
    "Since we know that this $\\mathbb{1}\\{X W_1 + b_1 > 0\\}$ will always be true as long as $\\mathbb{1}\\{H > 0\\}$ is true (since $H$ is already rectified), we can further simplify the formula by:\n",
    "\n",
    "$$\\begin{split}\n",
    "\\frac{\\partial L}{\\partial \\sigma} = \\frac{\\hat{Y} - Y}{N} W_2^T \\mathbb{1}\\{X W_1 + b_1 > 0\\} = \\frac{\\hat{Y} - Y}{N} W_2^T \\mathbb{1}\\{H > 0\\}\\\\\n",
    "\\end{split}$$\n",
    "\n",
    "Let's put this derivative in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_hidden_diff(H, W2, loss_deriv):\n",
    "    \"\"\"Computes the derivative of the first hidden layer, including it's activation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    H: np.ndarray\n",
    "        Output of the hidden layer.\n",
    "    W2: np.ndarray\n",
    "        Coefficients of the hidden to output layer.\n",
    "    loss_deriv: np.ndarray\n",
    "        Derivative of the output layer.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dhidden: np.ndarray\n",
    "        Derivative of the hidden layer.\n",
    "    \"\"\"\n",
    "    dhidden = np.dot(loss_deriv, W2.T)\n",
    "    dhidden[H <= 0] = 0  # ReLU backprop\n",
    "    return dhidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient of the trainable coefficients of the second layer \n",
    "\n",
    "Now that we have the derivative of the loss function with respect to the output (second layer), we can easily compute the gradient of the trainable coefficients $W_2$ and $b_2$ using backpropagation. Let's do it:\n",
    "\n",
    "$$\\begin{split}\n",
    "\\frac{\\partial L}{\\partial W_2} & = \\frac{\\partial L}{\\partial \\hat{Y}} \\frac{\\partial \\hat{Y}}{\\partial Z} \\frac{\\partial Z}{\\partial W_2} \\\\\n",
    "& = \\frac{\\hat{Y} - Y}{N} \\frac{\\partial Z}{\\partial W_2} \\\\\n",
    "& = \\frac{\\hat{Y} - Y}{N} \\frac{\\partial}{\\partial W_2} H W_2 + b_2 \\\\\n",
    "& = H^T \\frac{\\hat{Y} - Y}{N}\\\\\n",
    "\\end{split}$$\n",
    "\n",
    "$$\\begin{split}\n",
    "\\frac{\\partial L}{\\partial b_2} & = \\frac{\\partial L}{\\partial \\hat{Y}} \\frac{\\partial \\hat{Y}}{\\partial Z} \\frac{\\partial Z}{\\partial b_2} \\\\\n",
    "& = \\frac{\\hat{Y} - Y}{N} \\frac{\\partial}{\\partial b_2} H W_2 + b_2 \\\\\n",
    "& = \\frac{\\hat{Y} - Y}{N}\\\\\n",
    "\\end{split}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch 0: loss 0.738325\t acc 57.0%\n",
      "\tEpoch 1000: loss 0.598883\t acc 81.0%\n",
      "\tEpoch 2000: loss 0.566063\t acc 65.0%\n",
      "\tEpoch 3000: loss 0.347809\t acc 80.0%\n",
      "\tEpoch 4000: loss 0.201608\t acc 100.0%\n",
      "\tEpoch 5000: loss 0.123863\t acc 100.0%\n",
      "\tEpoch 6000: loss 0.083688\t acc 100.0%\n",
      "\tEpoch 7000: loss 0.066137\t acc 100.0%\n",
      "\tEpoch 8000: loss 0.044266\t acc 100.0%\n",
      "\tEpoch 9000: loss 0.032325\t acc 100.0%\n",
      "\tEpoch 10000: loss 0.030408\t acc 100.0%\n",
      "\tEpoch 11000: loss 0.021844\t acc 100.0%\n",
      "\tEpoch 12000: loss 0.019861\t acc 100.0%\n",
      "\tEpoch 13000: loss 0.015876\t acc 100.0%\n",
      "\tEpoch 14000: loss 0.016388\t acc 100.0%\n",
      "\tEpoch 15000: loss 0.013230\t acc 100.0%\n",
      "\tEpoch 16000: loss 0.011963\t acc 100.0%\n",
      "\tEpoch 17000: loss 0.011390\t acc 100.0%\n",
      "\tEpoch 18000: loss 0.010926\t acc 100.0%\n",
      "\tEpoch 19000: loss 0.008837\t acc 100.0%\n",
      "\tEpoch 20000: loss 0.009649\t acc 100.0%\n",
      "\tEpoch 21000: loss 0.008229\t acc 100.0%\n",
      "\tEpoch 22000: loss 0.008241\t acc 100.0%\n",
      "\tEpoch 23000: loss 0.006575\t acc 100.0%\n",
      "\tEpoch 24000: loss 0.007025\t acc 100.0%\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 25000\n",
    "step = 0.01\n",
    "for n_epoch in range(n_epochs):\n",
    "    # Forward pass of the network, catching the two layers' outputs\n",
    "    H, Y_est = forward_pass(X, W1, W2, b1, b2)\n",
    "    \n",
    "    # Loss: average cross-entropy loss\n",
    "    loss = compute_loss(Y, Y_est)\n",
    "    \n",
    "    # Accuracy\n",
    "    acc = np.sum(Y[np.arange(N), np.argmax(Y_est, axis=1)]) / float(N)\n",
    "    \n",
    "    # Printout\n",
    "    if n_epoch % 1000 == 0:\n",
    "        print(\"\\tEpoch %d: loss %f\\t acc %.1f%%\" % (n_epoch, loss, (acc * 100)))\n",
    "        \n",
    "    # Gradient of the second (output) layer\n",
    "    loss_deriv = compute_loss_diff(Y, Y_est)\n",
    "    \n",
    "    # Gradient of the hidden layer\n",
    "    dhidden = compute_hidden_diff(H, W2, loss_deriv)\n",
    "    \n",
    "    # Backpropagation of output layer\n",
    "    dW2 = np.dot(H.T, loss_deriv)\n",
    "    db2 = np.sum(loss_deriv, axis=0, keepdims=True)\n",
    "    \n",
    "    # Backprop of hidden layer\n",
    "    dW1 = np.dot(X.T, dhidden)\n",
    "    db1 = np.sum(dhidden, axis=0, keepdims=True)\n",
    "  \n",
    "    # Update step\n",
    "    W1 -= step * dW1\n",
    "    b1 -= step * db1\n",
    "    W2 -= step * dW2\n",
    "    b2 -= step * db2\n",
    "    \n",
    "    # Generate some new training data (this may help to avoid falling into local minima)\n",
    "    X, Y = gen_data(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in Test set: 100.00%\n"
     ]
    }
   ],
   "source": [
    "X_test, Y_test = gen_data(N)\n",
    "_, Y_est = forward_pass(X_test, W1, W2, b1, b2)\n",
    "acc = np.sum(Y_test[np.arange(N), np.argmax(Y_est, axis=1)]) / float(N)\n",
    "print(\"Accuracy in Test set: %.2f%%\" % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normal equations, solving with these four examples:\n",
    "X = np.array([[1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]])  # Added the bias as ones in the first column\n",
    "y = np.array([0, 1, 1, 0])\n",
    "b, w1, w2 = np.dot(np.linalg.inv(np.dot(X.T, X)), np.dot(X.T, y))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
